"""
Adaptive Learning System - Continuous Improvement Through Feedback
Implements prompt optimization, verification tuning, and user feedback processing
"""

from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
from collections import defaultdict
import json


@dataclass
class PromptPerformance:
    """Track performance of specific prompt strategies"""
    prompt_stage: str
    total_hypotheses: int
    successful_hypotheses: int
    success_rate: float
    avg_temperature: float
    last_updated: str


@dataclass
class VerificationWeights:
    """Current weights for verification layers"""
    static: float
    symbolic: float
    dynamic: float
    behavioral: float
    last_updated: str


@dataclass
class UserFeedback:
    """User feedback on vulnerability findings"""
    vulnerability_id: str
    feedback_type: str  # 'false_positive', 'confirmed', 'severity_adjustment'
    timestamp: str
    details: Dict[str, Any]
    pattern_name: Optional[str] = None


class PromptOptimizer:
    """
    Optimizes LLM prompts based on hypothesis quality
    Learns which prompt strategies generate high-quality hypotheses
    """
    
    def __init__(self):
        self.prompt_performance: Dict[str, PromptPerformance] = {}
        
    def track_hypothesis_result(self, 
                               prompt_stage: str,
                               temperature: float,
                               was_successful: bool):
        """Track result of a hypothesis generated by a prompt"""
        if prompt_stage not in self.prompt_performance:
            self.prompt_performance[prompt_stage] = PromptPerformance(
                prompt_stage=prompt_stage,
                total_hypotheses=0,
                successful_hypotheses=0,
                success_rate=0.0,
                avg_temperature=temperature,
                last_updated=datetime.now().isoformat()
            )
        
        perf = self.prompt_performance[prompt_stage]
        perf.total_hypotheses += 1
        
        if was_successful:
            perf.successful_hypotheses += 1
        
        # Update success rate
        perf.success_rate = perf.successful_hypotheses / perf.total_hypotheses
        
        # Update average temperature (running average)
        perf.avg_temperature = (
            (perf.avg_temperature * (perf.total_hypotheses - 1) + temperature) 
            / perf.total_hypotheses
        )
        perf.last_updated = datetime.now().isoformat()
        
    def get_optimized_temperature(self, prompt_stage: str) -> float:
        """Get optimized temperature for a prompt stage"""
        if prompt_stage not in self.prompt_performance:
            # Default temperatures by stage type
            defaults = {
                'divergent_exploration': 0.8,
                'creative_discovery': 0.75,
                'technical_validation': 0.3,
                'defensive_audit': 0.4
            }
            return defaults.get(prompt_stage, 0.6)
        
        perf = self.prompt_performance[prompt_stage]
        
        # Adjust temperature based on success rate
        if perf.total_hypotheses < 5:
            # Not enough data, use current
            return perf.avg_temperature
        
        if perf.success_rate > 0.7:
            # High success - can increase creativity slightly
            return min(1.0, perf.avg_temperature * 1.05)
        elif perf.success_rate < 0.3:
            # Low success - reduce temperature for more precision
            return max(0.1, perf.avg_temperature * 0.9)
        else:
            # Good performance, keep current
            return perf.avg_temperature
    
    def get_recommendations(self) -> List[str]:
        """Get recommendations for prompt improvements"""
        recommendations = []
        
        for stage, perf in self.prompt_performance.items():
            if perf.total_hypotheses < 5:
                continue
            
            if perf.success_rate < 0.3:
                recommendations.append(
                    f"Low success rate for {stage} ({perf.success_rate:.1%}). "
                    f"Reducing temperature to {self.get_optimized_temperature(stage):.2f}"
                )
            elif perf.success_rate > 0.8:
                recommendations.append(
                    f"High success rate for {stage} ({perf.success_rate:.1%}). "
                    f"Can increase creativity to {self.get_optimized_temperature(stage):.2f}"
                )
        
        return recommendations
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            stage: asdict(perf) 
            for stage, perf in self.prompt_performance.items()
        }
    
    def from_dict(self, data: Dict[str, Any]):
        """Load from dictionary"""
        self.prompt_performance = {
            stage: PromptPerformance(**perf_data)
            for stage, perf_data in data.items()
        }


class VerificationTuner:
    """
    Automatically tunes verification layer weights based on accuracy
    Adapts to which verification methods are most reliable
    """
    
    def __init__(self):
        self.layer_accuracy: Dict[str, List[float]] = {
            'static': [],
            'symbolic': [],
            'dynamic': [],
            'behavioral': []
        }
        self.current_weights = VerificationWeights(
            static=0.15,
            symbolic=0.25,
            dynamic=0.45,
            behavioral=0.15,
            last_updated=datetime.now().isoformat()
        )
        
    def record_layer_accuracy(self, layer: str, accuracy: float):
        """Record accuracy for a verification layer"""
        if layer in self.layer_accuracy:
            self.layer_accuracy[layer].append(accuracy)
    
    def adjust_weights(self, min_samples: int = 10):
        """Adjust weights based on recent performance"""
        if not any(len(acc) >= min_samples for acc in self.layer_accuracy.values()):
            # Not enough data yet
            return
        
        # Calculate recent accuracy (last 20 samples)
        recent_window = 20
        recent_accuracy = {}
        
        for layer in ['static', 'symbolic', 'dynamic', 'behavioral']:
            if len(self.layer_accuracy[layer]) > 0:
                recent_samples = self.layer_accuracy[layer][-recent_window:]
                recent_accuracy[layer] = sum(recent_samples) / len(recent_samples)
            else:
                recent_accuracy[layer] = 0.5  # Default
        
        # Adjust weights proportionally to accuracy
        new_weights = {}
        for layer in ['static', 'symbolic', 'dynamic', 'behavioral']:
            current_weight = getattr(self.current_weights, layer)
            accuracy = recent_accuracy[layer]
            
            # Increase weight for high-performing layers (>0.9 accuracy)
            if accuracy > 0.9 and len(self.layer_accuracy[layer]) >= min_samples:
                new_weights[layer] = current_weight * 1.1
            # Decrease weight for low-performing layers (<0.6 accuracy)
            elif accuracy < 0.6 and len(self.layer_accuracy[layer]) >= min_samples:
                new_weights[layer] = current_weight * 0.9
            else:
                new_weights[layer] = current_weight
        
        # Normalize weights to sum to 1.0
        total = sum(new_weights.values())
        for layer in new_weights:
            setattr(self.current_weights, layer, new_weights[layer] / total)
        
        self.current_weights.last_updated = datetime.now().isoformat()
    
    def get_weights(self) -> Dict[str, float]:
        """Get current weights as dictionary"""
        return {
            'static': self.current_weights.static,
            'symbolic': self.current_weights.symbolic,
            'dynamic': self.current_weights.dynamic,
            'behavioral': self.current_weights.behavioral
        }
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            'weights': asdict(self.current_weights),
            'accuracy_history': self.layer_accuracy
        }
    
    def from_dict(self, data: Dict[str, Any]):
        """Load from dictionary"""
        if 'weights' in data:
            self.current_weights = VerificationWeights(**data['weights'])
        if 'accuracy_history' in data:
            self.layer_accuracy = data['accuracy_history']


class PatternLearner:
    """
    Extracts new vulnerability patterns from successful detections
    Grows the pattern library automatically
    """
    
    def __init__(self):
        self.learned_patterns: List[Dict[str, Any]] = []
        self.pattern_signatures: set = set()  # Track unique patterns
        
    def extract_patterns(self, verified_vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Extract new patterns from verified vulnerabilities"""
        new_patterns = []
        
        for vuln in verified_vulnerabilities:
            # Generate pattern signature
            pattern_sig = self._generate_signature(vuln)
            
            # Check if pattern is novel
            if pattern_sig not in self.pattern_signatures:
                pattern = self._extract_pattern(vuln)
                new_patterns.append(pattern)
                self.learned_patterns.append(pattern)
                self.pattern_signatures.add(pattern_sig)
        
        return new_patterns
    
    def _generate_signature(self, vulnerability: Dict[str, Any]) -> str:
        """Generate unique signature for vulnerability pattern"""
        # Use vulnerability type and key characteristics
        vuln_type = vulnerability.get('name', vulnerability.get('type', 'unknown'))
        severity = vulnerability.get('severity', 'medium')
        return f"{vuln_type}:{severity}"
    
    def _extract_pattern(self, vulnerability: Dict[str, Any]) -> Dict[str, Any]:
        """Extract reusable pattern from vulnerability"""
        return {
            'name': vulnerability.get('name', vulnerability.get('type', 'unknown')),
            'severity': vulnerability.get('severity', 'medium'),
            'confidence': vulnerability.get('confidence', 0.5),
            'detection_strategy': vulnerability.get('detection_strategy', 'static_analysis'),
            'learned_at': datetime.now().isoformat(),
            'indicators': vulnerability.get('indicators', []),
            'code_signature': vulnerability.get('affected_code', '')[:100]
        }
    
    def get_patterns(self) -> List[Dict[str, Any]]:
        """Get all learned patterns"""
        return self.learned_patterns


class UserFeedbackProcessor:
    """
    Processes user feedback on findings to improve accuracy
    Adjusts pattern confidence and learning based on user input
    """
    
    def __init__(self, learning_db=None):
        self.feedback_log: List[UserFeedback] = []
        self.learning_db = learning_db
        
    def process_feedback(self,
                        vulnerability_id: str,
                        feedback_type: str,
                        pattern_name: str = None,
                        details: Dict[str, Any] = None) -> UserFeedback:
        """Process user feedback on a vulnerability finding"""
        feedback = UserFeedback(
            vulnerability_id=vulnerability_id,
            feedback_type=feedback_type,
            timestamp=datetime.now().isoformat(),
            details=details or {},
            pattern_name=pattern_name
        )
        
        self.feedback_log.append(feedback)
        
        # Apply feedback to learning database if available
        if self.learning_db and pattern_name:
            if feedback_type == 'false_positive':
                self._mark_false_positive(pattern_name)
            elif feedback_type == 'confirmed':
                self._mark_confirmed(pattern_name)
        
        return feedback
    
    def _mark_false_positive(self, pattern_name: str):
        """Reduce confidence for patterns marked as false positive"""
        if not self.learning_db:
            return
        
        if pattern_name in self.learning_db.pattern_effectiveness:
            stats = self.learning_db.pattern_effectiveness[pattern_name]
            # Record as false positive
            stats.false_positives += 1
            # Recalculate confidence
            total = stats.true_positives + stats.false_positives
            stats.confidence_score = stats.true_positives / total if total > 0 else 0.5
            stats.last_updated = datetime.now().isoformat()
    
    def _mark_confirmed(self, pattern_name: str):
        """Increase confidence for confirmed patterns"""
        if not self.learning_db:
            return
        
        if pattern_name in self.learning_db.pattern_effectiveness:
            stats = self.learning_db.pattern_effectiveness[pattern_name]
            # Record as true positive
            stats.true_positives += 1
            # Recalculate confidence
            total = stats.true_positives + stats.false_positives
            stats.confidence_score = stats.true_positives / total if total > 0 else 0.5
            stats.last_updated = datetime.now().isoformat()
    
    def get_feedback_summary(self) -> Dict[str, Any]:
        """Get summary of user feedback"""
        if not self.feedback_log:
            return {
                'total_feedback': 0,
                'message': 'No feedback recorded yet'
            }
        
        feedback_counts = defaultdict(int)
        for feedback in self.feedback_log:
            feedback_counts[feedback.feedback_type] += 1
        
        return {
            'total_feedback': len(self.feedback_log),
            'by_type': dict(feedback_counts),
            'recent_feedback': [asdict(f) for f in self.feedback_log[-5:]]
        }
    
    def to_dict(self) -> List[Dict[str, Any]]:
        """Convert to dictionary for storage"""
        return [asdict(f) for f in self.feedback_log]
    
    def from_dict(self, data: List[Dict[str, Any]]):
        """Load from dictionary"""
        self.feedback_log = [UserFeedback(**f) for f in data]


class AdaptiveLearningSystem:
    """
    Main adaptive learning system coordinating all components
    Continuously improves vulnerability detection through feedback
    """
    
    def __init__(self, learning_db=None):
        self.learning_db = learning_db
        self.prompt_optimizer = PromptOptimizer()
        self.verification_tuner = VerificationTuner()
        self.pattern_learner = PatternLearner()
        self.feedback_processor = UserFeedbackProcessor(learning_db)
        
    async def process_scan_results(self,
                                   scan_results: Dict[str, Any],
                                   user_feedback: Optional[Dict[str, Any]] = None):
        """
        Process scan results and improve system components
        This is the main feedback loop entry point
        """
        # Extract verified vulnerabilities
        verified_vulns = []
        if 'analysis_results' in scan_results:
            results = scan_results['analysis_results']
            
            # Collect all verified findings
            for pattern in results.get('novel_patterns', {}).get('patterns', []):
                if pattern.get('confidence', 0) > 0.7:
                    verified_vulns.append(pattern)
            
            for anomaly in results.get('anomalies', {}).get('anomalies', []):
                if anomaly.get('confidence', 0) > 0.7:
                    verified_vulns.append(anomaly)
        
        # Learn new patterns from verified findings
        new_patterns = self.pattern_learner.extract_patterns(verified_vulns)
        
        # Process user feedback if provided
        if user_feedback:
            for vuln_id, feedback_data in user_feedback.items():
                self.feedback_processor.process_feedback(
                    vulnerability_id=vuln_id,
                    feedback_type=feedback_data.get('type', 'confirmed'),
                    pattern_name=feedback_data.get('pattern_name'),
                    details=feedback_data.get('details', {})
                )
        
        # Adjust verification weights (simulate for now)
        # In real implementation, this would use actual verification results
        self.verification_tuner.adjust_weights()
        
        return {
            'new_patterns_learned': len(new_patterns),
            'feedback_processed': len(user_feedback) if user_feedback else 0,
            'current_verification_weights': self.verification_tuner.get_weights()
        }
    
    def get_comprehensive_metrics(self) -> Dict[str, Any]:
        """Get comprehensive metrics for all adaptive learning components"""
        return {
            'prompt_optimization': self.prompt_optimizer.to_dict(),
            'verification_tuning': self.verification_tuner.to_dict(),
            'pattern_learning': {
                'total_patterns_learned': len(self.pattern_learner.learned_patterns),
                'patterns': self.pattern_learner.get_patterns()
            },
            'user_feedback': self.feedback_processor.get_feedback_summary()
        }
    
    def save_state(self) -> Dict[str, Any]:
        """Save adaptive learning state for persistence"""
        return {
            'prompt_performance': self.prompt_optimizer.to_dict(),
            'verification_weights': self.verification_tuner.to_dict(),
            'learned_patterns': self.pattern_learner.get_patterns(),
            'user_feedback_log': self.feedback_processor.to_dict(),
            'last_updated': datetime.now().isoformat()
        }
    
    def load_state(self, state: Dict[str, Any]):
        """Load adaptive learning state from storage"""
        if 'prompt_performance' in state:
            self.prompt_optimizer.from_dict(state['prompt_performance'])
        
        if 'verification_weights' in state:
            self.verification_tuner.from_dict(state['verification_weights'])
        
        if 'learned_patterns' in state:
            self.pattern_learner.learned_patterns = state['learned_patterns']
            # Rebuild signatures
            self.pattern_learner.pattern_signatures = set(
                self.pattern_learner._generate_signature(p) 
                for p in state['learned_patterns']
            )
        
        if 'user_feedback_log' in state:
            self.feedback_processor.from_dict(state['user_feedback_log'])


# Global adaptive learning system instance
_global_adaptive_system = None


def get_adaptive_system(learning_db=None):
    """Get or create the global adaptive learning system"""
    global _global_adaptive_system
    if _global_adaptive_system is None:
        _global_adaptive_system = AdaptiveLearningSystem(learning_db)
    return _global_adaptive_system
